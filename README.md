# Natural language processing course: `Automatic generation of Slovenian traffic news for RTV Slovenija`

---

## Team members  
- **Janez TomÅ¡iÄ**  
- **Å½an PuÅ¡enjak**  
- **Matic ZadobovÅ¡ek**  

---

## ğŸ“‘ Table of contents
- [Natural language processing course: `Automatic generation of Slovenian traffic news for RTV Slovenija`](#natural-language-processing-course-automatic-generation-of-slovenian-traffic-news-for-rtv-slovenija)
  - [Team members](#team-members)
  - [ğŸ“‘ Table of contents](#-table-of-contents)
  - [ğŸ“‚ Project structure](#-project-structure)
    - [ğŸ“ `report/`](#-report)
    - [ğŸ“„ `environment.yml`](#-environmentyml)
    - [ğŸ“‚ `src/utils/`](#-srcutils)
    - [ğŸ“‚ `src/notebooks/`](#-srcnotebooks)
    - [ğŸ“‚ `src/evaluation/`](#-srcevaluation)
      - [`app/`](#app)
      - [`progress/`](#progress)
      - [`results/`](#results)
    - [ğŸ“‚ `src/arnes_hpc/`](#-srcarnes_hpc)
  - [ğŸ“Š Data](#-data)
  - [ğŸ” Reproducibility](#-reproducibility)
    - [ğŸ Local setup with Conda](#-local-setup-with-conda)
    - [ğŸ““ Notebooks](#-notebooks)
    - [ğŸŒ Streamlit app](#-streamlit-app)
    - [ğŸ’» ARNES HPC cluster setup](#-arnes-hpc-cluster-setup)
  - [ğŸ§ª Experiments](#-experiments)
    - [1ï¸âƒ£ Base-instructed (prompting only)](#1ï¸âƒ£-base-instructed-prompting-only)
    - [2ï¸âƒ£ Fine-Tuned](#2ï¸âƒ£-fine-tuned)
    - [3ï¸âƒ£ Fine-Tuned + instructed](#3ï¸âƒ£-fine-tuned--instructed)
    - [4ï¸âƒ£ Fine-tuned + instructed + RAG](#4ï¸âƒ£-fine-tuned--instructed--rag)
    - [âœ… Evaluation](#-evaluation)
      - [ğŸ“ˆ Automatic evaluation](#-automatic-evaluation)
      - [ğŸ‘¥ Manual evaluation](#-manual-evaluation)
  - [ğŸ“Š Results](#-results)
    - [ğŸ”¬ Automatic evaluation (SloBERTa cosine similarity)](#-automatic-evaluation-sloberta-cosine-similarity)
    - [ğŸ‘¥ Manual evaluation](#-manual-evaluation-1)
    - [ğŸ“ˆ F1 score distribution](#-f1-score-distribution)
---


## ğŸ“‚ Project structure

Below is an overview of the repository layout.

```
.
â”œâ”€â”€ report/
â”œâ”€â”€ environment.yml
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ notebooks/
â”‚   â”‚   â”œâ”€â”€ dataset_preparation.ipynb
â”‚   â”‚   â”œâ”€â”€ exploration.ipynb
â”‚   â”‚   â”œâ”€â”€ evaluation.ipynb
â”‚   â”‚   â”œâ”€â”€ finetuning_example.ipynb
â”‚   â”‚   â””â”€â”€ prompting.ipynb
â”‚   â”œâ”€â”€ evaluation/
â”‚   â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”‚   â””â”€â”€ app_evaluation.py
â”‚   â”‚   â”œâ”€â”€ progress/
â”‚   â”‚   â””â”€â”€ results/
â”‚   â””â”€â”€ arnes_hpc/
â”‚       â”œâ”€â”€ archive/
â”‚       â”œâ”€â”€ containers/
â”‚       â”œâ”€â”€ models/
â”‚       â”‚   â””â”€â”€ final_model_9b/
â”‚       â””â”€â”€ scripts/
â”‚           â”œâ”€â”€ finetuning.py
â”‚           â”œâ”€â”€ instructions.txt
â”‚           â”œâ”€â”€ run_base_model.py
â”‚           â”œâ”€â”€ run_finetuned_model.py
â”‚           â”œâ”€â”€ run_instructed_finetuned_model.py
â”‚           â”œâ”€â”€ run_rag_model.py
â”‚           â”œâ”€â”€ run_slurm_finetuning.sh
â”‚           â”œâ”€â”€ run_slurm_base_eval.sh
â”‚           â”œâ”€â”€ run_slurm_finetuned_eval.sh
â”‚           â”œâ”€â”€ run_slurm_instructed_eval.sh
â”‚           â””â”€â”€ run_slurm_rag_eval.sh
â”‚           â””â”€â”€ rag/
â”‚               â”œâ”€â”€ data_creation/
â”‚               â”œâ”€â”€ embed.py
â”‚               â”œâ”€â”€ rag_instructions_embeddings.npy
â”‚               â”œâ”€â”€ rag_instructions.jsonl
â”‚               â”œâ”€â”€ rag_roads_embeddings.npy
â”‚               â”œâ”€â”€ rag_roads.jsonl
â”‚               â””â”€â”€ retrieve_example.py
â””â”€â”€ README.md
```

---

### ğŸ“ `report/`  
PDFs of our project write-ups and submission reports.

---

### ğŸ“„ `environment.yml`  
Conda environment for local reproducibility (`conda env create -f environment.yml`).

---

### ğŸ“‚ `src/utils/`  
Helper functions for parsing, cleaning and structuring raw traffic data.

---

### ğŸ“‚ `src/notebooks/`  
Interactive Jupyter notebooks for:  
- **dataset_preparation.ipynb** â€” building and cleaning our dataset 
- **exploration.ipynb** â€” data inspection and exploratory data analysis  
- **evaluation.ipynb** â€” automatic metrics (SloBERTa score) 
- **finetuning_example.ipynb** â€” a toy LoRA run to run on Colab  
- **prompting.ipynb** â€” structured-prompt experiments  

---

### ğŸ“‚ `src/evaluation/`  
#### `app/`  
A Streamlit app (`app_evaluation.py`) we built to perform manual rating on test outputs.  
#### `progress/`  
Each memberâ€™s intermediate ratings.  
#### `results/`  
Final aggregated outputs for all **4 scenarios** (base-instructed, fine-tuned, fine-tuned + instructed, fine-tuned + instructed + RAG) on 500 examples.

---

### ğŸ“‚ `src/arnes_hpc/`  
Everything needed to run our **full experiments on the ARNES HPC cluster**:

- **archive/**: old / discarded models & scripts  
- **containers/**: Singularity definition files for building `.sif` images  
- **models/final_model_9b/**: the GaMS-9B checkpoint & LoRA adapter  
- **scripts/**:  
  - **finetuning.py**: QLoRA fine-tuning on H100  
  - **instructions.txt**: base-prompt rules  
  - **run_*.py** & **run_slurm_*.sh**: launch scripts for each of the 4 experiments  
  - **rag/**: embed & index road-chunks & instruction docs with LaBSE + retrieval demo

---

## ğŸ“Š Data

The dataset used in this project is **not included in this GitHub repository** due to size. However, you can download it from the following link:

ğŸ”— [Shared dataset folder (OneDrive)](https://unilj-my.sharepoint.com/:f:/g/personal/mz1034_student_uni-lj_si/Ev3111JpDF5GnWOgAbfmoj4B8Co1IlAyHUzjfHKZhusvjA?e=HB6Tvy)


The shared folder contains the original traffic report data, and also our processed training data.

We created a clean, structured JSONL file called `train_promet.jsonl`, which is used for fine-tuning the language model. Each entry is a JSON object with two keys:  
- `"prompt"` â€” a system-like input containing raw structured text  
- `"response"` â€” the corresponding expected radio-ready traffic report

---

## ğŸ” Reproducibility

Follow these instructions depending on the setup (local or HPC):

---

### ğŸ Local setup with Conda

You can start by creating the required environment using Conda. The provided `environment.yml` file will install all necessary dependencies under the environment name `nlp-project`. Later on we also provide separate instructions, for those who would prefer to manually install the needed dependencies.

```bash
conda env create -f environment.yml
conda activate nlp-project
```

---

### ğŸ““ Notebooks

All notebooks under `notebooks/` are designed for local execution â€” with the exception of `finetuning_example.ipynb`, which is a simplified demo version of LoRA fine-tuning, and is prepared for execution on Colab.

To run notebooks locally, make sure the following Python packages are installed (already included via `environment.yml`, but if you would prefer to do manual installation):

```bash
pip install beautifulsoup4 matplotlib numpy pandas seaborn striprtf ipython transformers torch scikit-learn
```

---

### ğŸŒ Streamlit app

To run the manual evaluation Streamlit app in `src/evaluation/app/app_evaluation.py`, you'll need:

```bash
pip install pandas streamlit
```

You can then launch the application using:
```bash
cd src/evaluation/app
streamlit run app_evaluation.py
```

---

### ğŸ’» ARNES HPC cluster setup

To run the full training or evaluation jobs on the ARNES HPC cluster:

1. Navigate to our project folder:
```bash
cd /d/hpc/projects/onj_fri/trije_konjeniki_apokalipse/
```

2. Launch jobs using:
```bash
sbatch run_slurm_<type>_eval.sh
```

The structure is modular â€” for each `.sh` (Slurm script) there's a corresponding Python script that performs the actual execution:

| SLURM script                   | Python script                      |
|-------------------------------|------------------------------------|
| `run_slurm_base_eval.sh`      | `run_base_model.py`               |
| `run_slurm_finetuned_eval.sh` | `run_finetuned_model.py`          |
| `run_slurm_instructed_eval.sh`| `run_instructed_finetuned_model.py`|
| `run_slurm_rag_eval.sh`       | `run_rag_model.py`                |

Each of these jobs produces a `.txt` file containing model outputs for 500 examples. These results are stored and used later for manual and automatic evaluation.

All the files required for running are already available under the same directory on HPC, so the workflow is fully reproducible and requires no extra setup.

---

## ğŸ§ª Experiments

We explored four experimental settings to see the effectiveness of prompt engineering, fine-tuning, and retrieval-augmented generation (RAG).

---

### 1ï¸âƒ£ Base-instructed (prompting only)

We used the original `cjvt/GaMS-9B-Instruct` model with structured prompting. The input format was designed with our defined rules, and no parameter updates were performed.

---

### 2ï¸âƒ£ Fine-Tuned

We performed **QLoRA** fine-tuning of the `cjvt/GaMS-9B-Instruct` model using our processed `train_promet.jsonl` dataset. The dataset was split 80/20 for training and validation.

We used:

- **Quantisation**: 4-bit NF4 with bfloat16 compute  
- **LoRA config**: `r=8`, `alpha=32`, `dropout=0.05`, targeting attention modules  
- **Batching**: `batch_size=1` with `gradient_accumulation=8`  
- **Max length**: 512 tokens  
- **Epochs**: 3  
- **Scheduler**: cosine with warmup  
- **Precision**: bfloat16  
- **Optimizer**: AdamW

The adapter and tokenizer were saved to disk for later inference.

---

### 3ï¸âƒ£ Fine-Tuned + instructed

We used the fine-tuned model, but kept the structured prompts to guide generation, essentially combining both approaches.

---

### 4ï¸âƒ£ Fine-tuned + instructed + RAG

We enhanced the instructed setup with **retrieval-augmented generation** using dense LaBSE embeddings. We embedded and indexed road and instruction snippets, retrieved the most relevant ones based on cosine similarity to input, and added them to the prompt.

---

### âœ… Evaluation

#### ğŸ“ˆ Automatic evaluation

We used **SloBERTa** and **cosine similarity** to score the model outputs against ground-truth references across all 4 settings on 500 test samples.

#### ğŸ‘¥ Manual evaluation

To get a better sense of model quality, we designed a **Streamlit-based web app** (`app_evaluation.py`) to allow all three of us to independently rank outputs of all 4 scenarios per example.

We followed this process:

1. **Pre-evaluation calibration** â€” we manually rated 50 **calibration examples** and compared our ranking differences to improve rating agreement. This helped us normalize our evaluation criteria and better understand nuances in generated outputs.

2. **Final evaluation** â€” we then independently rated **30 examples each**, across all four model variants.

For each example and scenario, we:

- Gave **a rating from 1 to 5**, assessing the overall usefulness and clarity.

- Compared each output directly to the ground-truth report, noting whether the generated output was better or worse.

The final scores are computed as a **global average** across all three of us for both criteria.

ğŸ–¼ï¸ The Streamlit app used for manual evaluation:

![Manual evaluation app](src/evaluation/app/evaluation_example.png)

Results from both evaluations are summarized in the next section.

---

## ğŸ“Š Results

We evaluated all four experimental setups using **two types of evaluation**:

- **Automatic evaluation** using **SloBERTa + cosine similarity** on 500 test examples.
- **Manual evaluation** of 30 test examples.

---

### ğŸ”¬ Automatic evaluation (SloBERTa cosine similarity)

| Model                          | Precision       | Recall          | F1-score        | Length difference (in words) |
|-------------------------------|-----------------|------------------|------------------|------------------------------|
| Base instructed                | 0.608 Â± 0.004   | 0.683 Â± 0.004    | 0.643 Â± 0.004    | 1.904 Â± 0.042                |
| Fine-tuned                    | 0.774 Â± 0.003   | 0.753 Â± 0.004    | 0.762 Â± 0.003    | 0.818 Â± 0.022                |
| Fine-tuned and instructed     | **0.817 Â± 0.003** | **0.752 Â± 0.004** | **0.781 Â± 0.003** | **0.732 Â± 0.017**            |
| Fine-tuned and instructed + RAG | 0.815 Â± 0.003   | 0.752 Â± 0.004    | 0.779 Â± 0.003    | 0.752 Â± 0.018                |

---

### ğŸ‘¥ Manual evaluation

| Scenario                      | Avg. Rating | % Outputs better than ground truth |
|------------------------------|-------------|------------------------------------|
| Base instructed              | 2.00        | 6.7%                               |
| Fine-tuned                  | 3.17        | 40.0%                              |
| Fine-tuned + instructed     | **3.37**    | **43.3%**                          |
| Fine-tuned + instructed + RAG | 3.30        | 40.0%                              |

---

### ğŸ“ˆ F1 score distribution

The histogram below shows the distribution of **F1 scores** across the 500 test examples, highlighting performance spread per method.

![F1 score distribution](src/evaluation/results/f1_distribution.png)

---

[â¬†ï¸ back to top](#-table-of-contents)